<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Instructions</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css">
    <link rel="stylesheet" href="{{ url_for('static', filename='instructions.css') }}">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
</head>
<body>
    <div class="container mt-5">
        <h2>Dear evaluator, welcome to the prompt evaluation schema</h2>
        <p>Please read the following instructions carefully before you begin your session</p>
        <ul>
            <li ><strong>Information</strong></li>
            <li><strong></strong> This is an evaluation of the outputs from a Large Language Model (LLM) based on some specific prompts</li>
            <li><strong></strong> You will only be presented with the outputs from the prompts, not the prompt itself</li>
            <li><strong></strong> Your will be presented with a subject, and a context in which the AI is aiming to explain</li>
            <li><strong></strong> These outputs will be generated uniquely for you</li>
            <li><strong>Your Task</strong></li>
            <li><strong></strong> Rate the response based on correctness, relevance, appropriateness, and clarity.</li>
            <li><strong></strong> You will be presented with 4 outputs from the AI for each evaluation task</li>
            <li><strong></strong> If you encounter that an evalation task presents the text "No response available, rate everything to 1 and rank lowest, then continue</li>
            <li><strong></strong> Rank the outputs from 1-4, where 1 is the output you rate the highest</li>
            <li><strong></strong> You can skip a response if you are unsure about the evaluation, but you have to interact with all the metrics in order to proceed to the next evaluation task</li>
            <li><strong></strong> Your evaluations will help to determine what kind of prompts are most benificial for learning.</li>
            <li><strong></strong> Below are the definitions of the evalation metrics ordinal scale. Please read them carefully.</li>
            <li><strong></strong> During the evaluation you can always access the definitions of the evaluation metrics by pressing the 'info' button at the bottom of the screen</li>
            <li><strong></strong> Thank you for participating in this experiment</li>
        </ul>
    </div>
    <div class="container mt-5">
        <h3>Evaluation Metrics Scale</h3>
        <p>This section describes the metrics used to evaluate AI-generated responses</p>
    
        <div>
            <h3>1. Correctness (Factual Accuracy)</h3>
            <p>Correctness measures the accuracy and truthfulness of the response</p>
            <ul>
                <li><strong>1 (Inaccurate):</strong> The response contains significant factual inaccuracies or is fundamentally incorrect.</li>
                <li><strong>2 (Mostly Inaccurate):</strong> The response is mostly incorrect but may contain isolated correct elements.</li>
                <li><strong>3 (Partially Accurate):</strong> The response includes both correct and incorrect information.</li>
                <li><strong>4 (Mostly Accurate):</strong> The response is mainly correct, with minor inaccuracies.</li>
                <li><strong>5 (Accurate):</strong> The response is entirely correct and free from factual errors.</li>
            </ul>
        </div>
        <div>
            <h3>2. Relevance</h3>
            <p>Relevance determines whether the response is directly related to the subject</p>
            <ul>
                <li><strong>1 (Irrelevant):</strong> The response has little to no relevance to the subject.</li>
                <li><strong>2 (Barely Relevant):</strong> The response is only tangentially related.</li>
                <li><strong>3 (Moderately Relevant):</strong> The response is generally relevant but may contain some off-topic information.</li>
                <li><strong>4 (Highly Relevant):</strong> The response is closely aligned with the subject.</li>
                <li><strong>5 (Completely Relevant):</strong> The response is entirely on-topic.</li>
            </ul>
        </div>
        <div>
            <h3>3. Appropriateness</h3>
            <p>Appropriateness evaluates whether the response is suitable for the given context</p>
            <ul>
                <li><strong>1 (Inappropriate):</strong> The response is entirely unsuitable for the context, potentially offensive.</li>
                <li><strong>2 (Marginally Appropriate):</strong> The response has slight appropriateness but leans heavily toward one end of the spectrum.</li>
                <li><strong>3 (Moderately Appropriate):</strong> The response is moderately suitable but might not fully satisfy the intended audience.</li>
                <li><strong>4 (Highly Appropriate):</strong> The response is well-suited for the context.</li>
                <li><strong>5 (Optimally Appropriate):</strong> The response strikes the ideal balance.</li>
            </ul>
        </div>
        <div>
            <h3>4. Clarity</h3>
            <p>Clarity measures how easily the response can be understood</p>
            <ul>
                <li><strong>1 (Unclear):</strong> The response is difficult to understand.</li>
                <li><strong>2 (Slightly Clear):</strong> The response has some clarity but may require significant effort to understand.</li>
                <li><strong>3 (Moderately Clear):</strong> The response is generally clear but may contain some ambiguities.</li>
                <li><strong>4 (Highly Clear):</strong> The response is easy to understand.</li>
                <li><strong>5 (Completely Clear):</strong> The response is exceptionally clear.</li>
            </ul>
        </div>
        <button class="btn btn-primary" type="button" id="startEvaluationButton" disabled>Start Evaluation</button>
    </div>
    

    <script>
        $(document).ready(function() {
          const startEvaluationButton = $('#startEvaluationButton');
    
          // Disable button for 30 seconds - This is to give time fo the API to fetch the data
          startEvaluationButton.prop('disabled', true);
          let timer = setTimeout(function() {
            startEvaluationButton.prop('disabled', false); // Enable button after 15 seconds
          }, 30000); // 30 seconds in milliseconds
    
          // Redirect on click (after timer completes)
          startEvaluationButton.click(function() {
            window.location.href = '/evaluation';
          });
        });
      </script>
</body>
</html>
